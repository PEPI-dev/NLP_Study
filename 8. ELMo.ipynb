{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMGnat/Mb/awd9VJ38hkgsf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 1. ELMo(Embeddings from Language Model)\n","- 2018년 [논문](https://arxiv.org/abs/1802.05365)에서 제안된 새로운 워드 임베딩 방법론\n","- [ELMo](https://wikidocs.net/33930)의 가장 큰 특징은 사전 훈련된 언어 모델(Pre-Trained Language Model)을 사용한다는 것"],"metadata":{"id":"kjpl4qjWkR6O"}},{"cell_type":"markdown","source":["# 2. ELMo의 특징\n","- 기존 워드 임베딩은 주변 문맥 정보를 활용하여 단어를 벡터로 표현하는 방법을 사용하였음\n","- biLM이라는 구조를 사용 -> N개의 tokens으로 이루어진 입력 문장을 양방향(forward, backward)의 언어 모델링을 통해 문맥적인 표현을 반영하여 해당 입력 문장의 확률을 예측\n","- 사전 학습된 단어 표현을 사용 -> 대량의 자연어 코퍼스를 미리 학습하여 코퍼스 안에 포함된 일반화된 언어 특성들을 모델의 파라미터 안에 함축하는 방법을 사용"],"metadata":{"id":"7QRf_oqGk5QZ"}},{"cell_type":"markdown","source":["# 3. ELMo 요약\n","- ELMo는 기존의 단어 임베딩 구조가 문맥의 정보를 충분히 반영하지 못한다는 한계를 지적\n","- 양방향 학습이 가능한 biLM으로부터 문맥 내 정보를 충분히 반영하는 문장 벡터 표현을 학습하는 일반적인 방법을 소개\n","- ELMo를 적용했을 때 많은 성능 향상을 가져왔으며 학습데이터가 작을수록 더 효율적인 학습이 가능"],"metadata":{"id":"lnAki4Q7nQC5"}},{"cell_type":"code","source":[],"metadata":{"id":"4ttYAX6Gnydq"},"execution_count":null,"outputs":[]}]}