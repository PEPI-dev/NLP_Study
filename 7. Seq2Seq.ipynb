{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOZ0t+kAhlnVtU46rn7T6tR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 1. 문장 임베딩\n","- 2017년 이전의 임베딩 기법들은 대부분 단어 수준의 모델이었음(Word2Vec, FastText , Glove)\n","- 단어 수준 임베딩 기법은 자연어의 특성인 모호성, 동음이의어를 구분하기 어렵다는 한계가 있음\n","- 2017년 이후에는 ElMo(Embeddings from Language Models)와 같은 모델이 발표되고 트랜스포머와 같은 언어 모델에서 문장수준의 언어 모델링을 고려하면서 한계점들이 해결됨"],"metadata":{"id":"Noqa9xYfIGts"}},{"cell_type":"markdown","source":["### 1-1. 언어 모델\n","- 자연어처리 작업은 자연어 문장을 생성하거나 예측하는 방식으로 결과를 표현함\n","- 자연어처리 작업에서는 자연어를 수치화하여 표현할 수 있는 언어모델을 사용\n","- 언어 모델은 자연어 문장 혹은 단어에 확률을 할당하여 컴퓨터가 처리할 수 있도록 하는 모델로 주어진 입력에 대해 가장 자연스러운 단어 시퀀스를 찾을 수 있음"],"metadata":{"id":"tH5LuF0cJAJE"}},{"cell_type":"markdown","source":["### 1-2. 언어 모델링\n","- 주어진 단어들로부터 아직 모르는 단어들을 예측하는 작업\n","- 사람은 수많은 단어와 문장을 듣고 쓰고 말하며 언어 능력을 학습해왔기 때문에 문장 구성 및 의미상 가장 적합한 단어를 판단할 수 있음 -> 문장을 기계에서 보여주고 적합한 단어를 예측하도록 학습\n","(기계도 사람과 같은 프로세스로 동작함)\n","- 언어 모델은 텍스트 기반의 수많은 문장을 통해 어떤 단어가 어떤 어순으로 쓰인 것이 가장 자연스러운 문장인지 학습함"],"metadata":{"id":"N8aeif2CJdzB"}},{"cell_type":"markdown","source":["# 2. 자연어처리 모델 구조\n","- 자연어처리 분야의 인공지능 모델은 근 10년 동안 수 많은 모델 구조에 걸쳐 진화해 왔음\n","- 사용되지 않는 모델도 있으나 해당 모델이 나온 이유와 구조, 한계점들을 공부하는 것이 자연어처리 분야에서 통찰을 얻는데 많은 도움이 됨\n","- 현재에도 분야별로 다양한 모델들이 공개되고 있는데 자연어 처리에서 핵심은 공개된 모델들 중 어떤 언어 모델이 내가 풀고자 하는 문제에 가장 적합한지 탐색하는 것\n","- 대부분의 분야에서 트랜스포머 계열의 모델이 가장 인기 있지만, 특정 자연어 작업 처리에 특화된 세부적인 테크닉들이 다르므로 최신 연구 동향과 SOTA 모델들을 팔로업하는 것이 좋음"],"metadata":{"id":"h9I5LvJZKChC"}},{"cell_type":"markdown","source":["### 2-1. 자연어처리 분야의 주요 언어 모델\n","\n","- Seq2Seq\n","- ELMO\n","- Transformer\n","- GPT\n","- BERT"],"metadata":{"id":"w7bvmMRpLstE"}},{"cell_type":"markdown","source":["### 2-2. Seq2Seq 배경\n","- Seq2Seq 모델이 등장하기 전에 DNN(Deep Neural Network) 모델은 사물 인식, 음성 인식 등에서 꾸준히 성과를 냈음(예 : CNN,RNN, LSTM, GRU...)\n","- 모델 입/출력의 크기가 고정된다는 한계점이 존재했기 때문에 자연어처리와 같은 가변적인 길이의 입/출력을 처리하는 문제들을 제대로 해결할 수 없었음\n","- RNN은 Seq2Seq가 등장하기 전에 입/출력을 시퀀스 단위로 처리할 수 있는 모델이었음"],"metadata":{"id":"f33po7hXNI9y"}},{"cell_type":"markdown","source":["### 2-3. Seq2Seq(Sequence To Sequence)란\n","- 2014년 구글에서 [논문](https://arxiv.org/abs/1409.3215)으로 제안한 모델\n","- [LSTM(Long Short-Term Memory)](https://wikidocs.net/22888) 또는 [GRU(Gated Recurrent Unit)](https://wikidocs.net/22889) 기반의 구조를 가지고 고정된 길이의 단어 시퀀스를 입력받아 입력 시퀀스에 알맞은 길이의 시퀀스를 출력해주는 언어 모델\n","- Seq2Seq는 2개의 LSTM을 각각 Encoder와 Decode로 사용해 가변적인 길이의 입출력을 처리하고자 했음\n","- 기계 번역 작업에서 큰 성능 향상을 가져왔고, 특히 긴 문장을 처리하는데 강점이 있음"],"metadata":{"id":"t49PGb56OyuV"}},{"cell_type":"markdown","source":["### 2-4. Seq2Seq 모델 구조\n","- Seq2Seq는 한 문장을 다른 문장으로 변환하는 모델\n","- 가변길이의 입/출력을 처리하기 위해 인코더, 디코더 구조를 채택\n","- 인코더와 디코더는 모두 여러 개의 LSTM 또는 GRU셀로 구성되어 있음\n","- RNN 대신 LSTM 또는 GRU 셀을 사용하는 이유는 Long-term Dependency를 해결하기 위해 사용"],"metadata":{"id":"9kT8zIE5PGR0"}},{"cell_type":"markdown","source":["### 2-5. 인코더\n","- 입력 문장을 컨텍스트 벡터에 압축하는 역할을 함\n","- 인코더의 LSTM은 입력 문장을 단어 순서대로 처리하여 고정된 크기의 컨텍스트 벡터를 반환\n","- 컨텍스트 벡터는 인코더의 마지막 스텝에서 출력된 hidden state와 같음\n","- 컨텍스트 벡터는 입력 문장의 정보를 함축하는 벡터이므로 해당 벡터를 입력 문장에 대한 수준의 벡터로 활용될 수 있음"],"metadata":{"id":"mQV_i26IWAZo"}},{"cell_type":"markdown","source":["### 2-6. 디코더  \n","- 입력 문장의 정보가 압축된 컨텍스트 벡터를 사용하여 출력 문장을 디코딩하는 역할\n","- 컨텍스트 벡터와 문장의 시작을 뜻하는<sos>토큰을 입력으로 받아서, 문장의 끝을 뜻하는 <eos>토큰이 나올 떄 까지 문장을 생성\n","- LSTM의 첫 셀에서는 <sos>토큰과 컨텍스트 벡터를 입력받아서 그 다음에 등장할 확률이 가장 높은 단어를 예측하ㅗ 다음 스탭에서 예측한 단어를 입력으로 받아서 그 다음에 등장할 확률이 가장 높은 단어를 예측하는 형태로 계속 진행\n","- 토큰이 나오면 생성을 종료"],"metadata":{"id":"xfnYkLm4YiPX"}},{"cell_type":"markdown","source":["### 2-7. 학습과정과 한계\n","- 모델 학습 과정에서는 이전 셀에서 예측한 단어를 다음 셀의 입력으로 넣어주는 대신 실제 정답 단어를 다음 셀의 입력으로 넣음\n","- 만약 위 방법으로 학습되지 않으면 이전 셀에서의 오류가 다음 셀로 계속 전파될 것이고 그러면 학습이 제대로 되지 않음\n","- 가변적인 길이의 입/출력을 처리하는데 효과적인 모델 구조이며, 실제로 기계 번역 작업에서 성능 향상을 거뒀으나 여전히 한계를 가짐\n","- 인코더가 출력하는 벡터 사이즈가 고정되어 있기 때문에 입력으로 들어오는 단어의 수가 매우 많아지면 성능이 떨어짐\n","- RNN 구조의 모델에서는 hidden sate를 통해 이전 셀의 정보를 당므 셀로 계속 전달하게 되는데 문장의 길이가 길어지면 초기 셀에서 전달됬던 저보들이 점차 흐려짐\n","- LSTM, GRU 같은 모델들이 제안되긴 했으나 여전히 이전 정보를 계속 압축하는데 한계는 있음"],"metadata":{"id":"MhLwxGXtZNxj"}},{"cell_type":"markdown","source":["# 3. 어텐션 메커니즘\n","- Seq2Seq 모델의 한계를 해결하기 위해 제안한 논문에서 발표\n","- Attention 이라는 단어가 쓰이지 않았지만 Attention 개념을 제공한 연구 [논문](https://arxiv.org/abs/1409.0473)\n","- Attention 단어를 사용한 모델에 대한 연구 [논문](https://arxiv.org/abs/1508.04025)\n","- 입력 시퀀스가 길어지면 출력 시퀀스의 정확도가 떨어지는 것을 보정해주기 위해 등장한 기법\n","- 어텐션의 아이디어는 디코더에서 출력단어를 예측하는 매 시점(time step)마다 인코더에서의 전체 입력 문장을 다시 한 번 참고하는 것\n","- 단, 전체 입력 문장을 전부 다 동일한 비율로 참고하는 것이 아니라, 해당 시점에서 예측해야 할 단어와 연관이 있는 입력 받어 부분을 좀 더 집중해서 보게 함"],"metadata":{"id":"ITcXXzG3agKX"}},{"cell_type":"markdown","source":["### 3-1. 어텐션 함수\n","- 어텐션 함수는 주어진 쿼리에 대하여 모든 키와의 유사도를 각각 계산\n","- 계산도니 유사도를 키와 맵핑되어 있는 각각의 값에 반영한 뒤 유사도가 반영된 값을 모두 더해서 반환(어텐션 값)"],"metadata":{"id":"bdxS5SnRbTuC"}},{"cell_type":"markdown","source":["### 3-2. 어텐션과 Seq2Seq\n","- 어텐션 매커니즘은 Seq2Seq 모델이 가지는 한계를 해결하기 위해 제안되었기 때문에 Seq2Seq 모델에 어텐션 메커니즘을 적용한 모델을 제안\n","- Q = Query : t시점의 디코더 셀에서의 은닉 상태\n","- K = Keys  : 모든 시점의 인코더 셀의 은닉 상태들\n","- V = Values : 모든 시점의 인코더 셀의 값"],"metadata":{"id":"7lMGolRofNwI"}},{"cell_type":"markdown","source":["### 3-3. 어텐션의 작동 원리\n","- 디코더의 첫번쨰, 두번째 LSTM셀은 어텐션 메커니즘을 통해 단어를 예측하는 과정을 거쳤다고 가정하고 디코더에 세번쨰 LSTM 셀은 출력 단어를 예측하기 위해 이코더의 모든 입력 단어들의 정보를 다시 참고\n","- 어텐션 스코어를 구하는데 이는 hidden state 벡터 간 dot product를 사용하여 계산함"],"metadata":{"id":"lW3CJATWfyk8"}},{"cell_type":"code","source":[],"metadata":{"id":"7F1Pg7jIgWRF"},"execution_count":null,"outputs":[]}]}